{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Home Work 3\n",
    "## Linear Regression: Overfitting and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will see, by examples, how the linear models are overfitting, we will understand why this happens, and we will find out how to diagnose and control overfitting.\n",
    "\n",
    "In all cells where the comment with instructions was written, you need to write the code that executes these instructions. The rest of the cells with the code (without comments) just need to be executed. In addition, the assignment requires answering questions; The answers must be entered after the selected word \"Answer:\".\n",
    "\n",
    "We remind you that you can use the combination Shift + Tab to see the help of any method or function (find out what arguments it has and what it does). Clicking Tab after the object name and point allows you to see what methods and variables this object has.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with the __\"bikes_rent.csv\"__ dateset, in which calendar information and weather conditions characterizing the automated bicycle rental points are recorded daily, as well as the number of rentals that day. The latter will be predicted; Thus, we will solve the problem of regression. https://www.kaggle.com/c/bike-sharing-demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset using the __pandas.read_csv__ function into the __df__ variable. Output the first 5 lines to make sure that the data is correctly read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data and output the first 5 lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each rental day, the following characteristics are known :\n",
    "\n",
    "***season*** :  1 = spring, 2 = summer, 3 = fall, 4 = winter \n",
    "\n",
    "***yr*** : 0 - 2011, 1 - 2012\n",
    "\n",
    "***mnth*** : from 1 to 12\n",
    "\n",
    "***holiday*** : whether the day is considered a holiday\n",
    "\n",
    "***weekday*** : from 0 to 6\n",
    "\n",
    "***workingday*** : whether the day is neither a weekend nor holiday\n",
    "\n",
    "***weathersit*** : 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "    \n",
    "***temp*** : temperature in Celsius\n",
    "\n",
    "***atemp*** : \"feels like\" temperature in Celsius\n",
    "\n",
    "***hum*** : relative humidity\n",
    "\n",
    "***windspeed(mph)*** :   wind speed mph\n",
    "\n",
    "***windspeed(ms)*** :   wind speed ms\n",
    "\n",
    "***cnt*** : number of total rentals\n",
    "\n",
    "\n",
    "So, we have real, binary and nominal (ordinal) signs, and with all of them you can work with both material. With nominal signs, you can also work as a material, because they are given an order. Let's look at the charts, how the target attribute depends on the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 10))\n",
    "for idx, feature in enumerate(df.columns[:-1]):\n",
    "    df.plot(feature, \"cnt\", subplots=True, kind=\"scatter\", ax=axes[idx / 4, idx % 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Block 1. Answer the questions: __\n",
    "1. What is the nature of the dependence of the number of rent a month?\n",
    "    * answer:\n",
    "1. Specify one or two characteristics, from which the number of rentals is most likely linearly dependent\n",
    "    * answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's more strictly estimate the level of linear dependence between the characteristics and the target variable. A good measure of the linear relationship between the two vectors is the Pearson correlation. In pandas, it can be calculated using two methods of a dataframe: corr and corrwith. The df.corr method calculates the correlation matrix of all characteristics from the dataframe. The df.corrwith method needs to submit one more dataframe as an argument, and then it will calculate pairwise correlations between the characteristics of df and this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the correlation of all attributes except the last, with the latter using the corrwith method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample, there are signs that correlate with the target, and therefore the problem can be solved by linear methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs show that some of the signs are similar to each other. Therefore, let us also calculate the correlation between the numerical examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the pairwise correlations between the temp, atemp, hum, windspeed (mph), windspeed (ms), and cnt\n",
    "# using the corr method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the diagonals, as it should be, there are only one. However, there are two more pairs of strongly correlated columns in the matrix: temp and atemp (correlate by nature) and two windspeed columns (because this is simply the translation of some units into others). Further we will see that this fact negatively affects the learning of the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at the average characteristics (the mean method) in order to estimate the scale of features and the proportion of 1 for binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output average characteristics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples have a different scale, which means that for further work we better normalize the matrix of the objects-features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem one: collinear features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in our data, one feature duplicates the other, and there are two very similar ones. Of course, we could remove duplicates at once, but let's see how the model would have been trained if we had not noticed this problem.\n",
    "\n",
    "First, we scale, or standardize the characteristics: from each feature, subtract its average and divide by the standard deviation. This can be done with the scale method.\n",
    "\n",
    "In addition, you need to shufle the sample, this is required for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = shuffle(df, random_state=123)\n",
    "X = scale(df_shuffled[df_shuffled.columns[:-1]])\n",
    "y = df_shuffled[\"cnt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the linear regression on our data and look at the weights of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a linear regressor object, train it on all data and output the weights of the model\n",
    "# (weights are stored in the coef_ variable of the regressor class).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can output pairs (attribute name, weight) using the zip function built into the python language\n",
    "# The characteristic names are stored in the variable df.columns\n",
    "#temp=pd.DataFrame(list(zip(df.columns,np.around(lin_reg.coef_, decimals=3))))\n",
    "#temp.index.name='№'\n",
    "#temp.columns=['features', 'weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the weights for linearly dependent characteristics are significantly larger in modulus than in other characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why this happened, let us recall the analytical formula by which the weights of the linear model in the method of least squares are calculated:\n",
    "\n",
    "$ w = (X ^ TX) ^ {- 1} X ^ T y $.\n",
    "\n",
    "If in X there are collinear (linearly dependent) columns, the matrix $ X ^ TX $ becomes degenerate, and the formula ceases to be correct. The more dependent the characteristics, the smaller the determinant of this matrix and the worse the approximation of $ Xw \\approx y $. Such a situation is called the problem of _multicollinearity_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a pair of temp-atemp slightly less correlating variables, this did not happen, but in practice it is always worthwhile to closely monitor the coefficients at similar characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The solution__ of the multicollinearity problem consists in the _regularization_ of the linear model. To the optimized functional add L1 or L2 the norm of the weights multiplied by the regularization coefficient $\\alpha$. In the first case, the method is called Lasso, and in the second, Ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teach the Ridge and Lasso regressors with default parameters and make sure the problem with the scales is resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear model with L1-regularization and output weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear model with L2-regularization and output weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second problem: non-informative features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to L2-regularization, L1 nullifies weights at some traits. \n",
    "\n",
    "Let's observe how the weights change with increasing regularization factor $\\alpha$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = np.arange(1, 500, 50)\n",
    "coefs_lasso = np.zeros((alphas.shape[0], X.shape[1])) # matrix of weights of size (number of regressors) x (number of features)\n",
    "coefs_ridge = np.zeros((alphas.shape[0], X.shape[1]))\n",
    "# For each coefficient value from alphas, train the Lasso regressor\n",
    "# and write down the weights in the corresponding row of the coefs_lasso matrix (remember the built-in python function enumerate)\n",
    "# and then train the Ridge and write down the weights in coefs_ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the dynamics of weights with increasing regularization parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "for coef, feature in zip(coefs_lasso.T, df.columns):\n",
    "    plt.plot(alphas, coef, label=feature, color=np.random.rand(3))\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.4, 0.95))\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"feature weight\")\n",
    "plt.title(\"Lasso\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for coef, feature in zip(coefs_ridge.T, df.columns):\n",
    "    plt.plot(alphas, coef, label=feature, color=np.random.rand(3))\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.4, 0.95))\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"feature weight\")\n",
    "plt.title(\"Ridge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers to the following questions can be given by looking at the graphs or by outputting the coefficients for printing.\n",
    "\n",
    "__ Block 2. Answer the questions  __:\n",
    "1. Which regularizer (Ridge or Lasso) aggressively reduces weight for the same alpha?\n",
    "    * Answer:\n",
    "1. What happens to the Lasso weights, if alpha is made very large? Explain why this happens.\n",
    "    * Answer:\n",
    "1. Is it possible to assert that Lasso excludes one of the feature of windspeed for any value of alpha> 0? And Ridge? It is assumed that the regularizer excludes the attribute if the coefficient of it is <1e-3.\n",
    "    * Answer:\n",
    "1. Which of the regularizers is suitable for selecting non-informative features?\n",
    "    * Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further we will work with Lasso.\n",
    "\n",
    "So, we see that when the alpha is changed, the model selects the coefficients of the features in different ways. We need to choose the best alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, first, we need a quality metric. We will use the optimized functional of the method of least squares, that is, Mean Square Error, as the metric.\n",
    "\n",
    "Secondly, it is necessary to understand what data this metric is to be counted on. You can not select alpha by the MSE value on the training sample, because then we will not be able to estimate how the model will make predictions on the new data for it. If we select one splitting of the sample into a training and test (called holdout), then we will adjust to specific \"new\" data, and again we can retrain. Therefore, we will make several partitions of the sample, at each try different values ​​of alpha, and then average the MSE. It is most convenient to make such cross-sections by cross-validation, that is, to divide the sample into K parts or blocks, and each time to take one of them as a test, and from the remaining blocks make up a training sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do cross-validation for regression in sklearn is quite simple: for this there is a special regressor, __LassoCV__, which takes an alpha list on the input and calculates MSE for cross-validation for each of them. After the training (if you leave the parameter cv = 3 by default), the regressor will contain the variable __mse\\_path\\___, the matrix of the size len (alpha) x k, k = 3 (the number of blocks in the cross-validation) containing the MSE values on the test for the corresponding starts . In addition, in the variable alpha\\_ the selected value of the regularization parameter will be stored, and in coef\\_, traditionally, the weights corresponding to this alpha_ will be stored.\n",
    "\n",
    "Note that the regressor can change the order in which it passes through alphas; It is better to use the variable regressor alphas_ to match the MSE matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Traine the LassoCV regressor on all regularization parameters from alpha\n",
    "# Construct a graph of _ averaged_ over MSE lines depending on alpha \n",
    "# Output the selected alpha, as well as the \"feature-coefficient\" pairs for the trained coefficient vector\n",
    "alphas = np.arange(1, 100, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have chosen some regularization parameter. Let's see what we would choose alpha if we only split the sample once into the training and test, that is, consider the MSE trajectories corresponding to the individual sampling blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output the alpha values corresponding to the MSE minima on each partition (that is, by the columns).\n",
    "# On three separate graphs, visualize the columns .mse_path_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each partition, the optimal value of alpha is its own, and it corresponds to a large MSE on other partitions. It turns out that we are tuning to specific training and control samples. When choosing alpha for cross-validation, we choose something \"average\", which will give an acceptable value for the metric on different partitions of the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as is the case in data analysis, let's interpret the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Block 3. Answer the questions: __\n",
    "1. In the last trained model, select the 4 features with the largest (positive) coefficients (and write them out), look at the visualization of cnt dependencies on these attributes, which we drew in the \"Introduction to Data\" block. Is the increasing linear dependence of cnt on these characteristics from the graphs visible? Is it logical to argue (out of common sense) that the greater the significance of these signs, the more people will want to take bicycles?\n",
    "    * Answer:\n",
    "1. Select 3 features with the largest negative coefficients (and write them out), look at the corresponding visualizations. Is a decreasing linear dependence visible? Is it logical to argue that the greater the magnitude of these signs, the less people will want to take bicycles?\n",
    "    * Answer:\n",
    "1. Write down the features with coefficients close to zero (<1e-3). Why do you think the model excluded them from the model (again look at the charts)? Is it true that they do not in any way affect the demand for bicycles?\n",
    "    * Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "So, we looked at how you can monitor the adequacy of the linear model, how to select the attributes and how to correctly, if possible, not adjusting to any particular portion of the data, to select the regularization coefficient.\n",
    "\n",
    "It is worth noting that using cross-validation it is convenient to select only a small number of parameters (1, 2, maximum 3), because for each allowed combination of them we have to train the model several times, and this is a time-consuming process, especially if it is necessary to be trained on large volumes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
